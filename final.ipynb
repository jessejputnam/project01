{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big Question\n",
    "- Which shippers are most/least reliable (arrival time delta between estimated and actual)?\n",
    "\n",
    "#### Sub Questions\n",
    "- Which are the most reliable shippers per country/region/subregion\n",
    "- Which carrier companies are the most reliable?\n",
    "- What, if any, were the reliability changes over the years?\n",
    "    - How did covid affect reliability metrics of shipment times?\n",
    "- Which consignees chose their shippers wisest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lib.utils import get_id_nums, clean_row, remove_incorrect_codes, get_data_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get table of countries with alpha-2 code that includes region from repository\n",
    "    - https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_cols = ['name', 'alpha-2', 'region', 'sub-region']\n",
    "# keep_default_na=False because Namibia's code is NA\n",
    "countries = pd.read_csv(\n",
    "    get_data_url('bronze', 'country_data'), \n",
    "    usecols=countries_cols, \n",
    "    keep_default_na=False\n",
    ")\n",
    "\n",
    "# Extra codes in data missing from table\n",
    "extra_codes = pd.DataFrame(\n",
    "    {\n",
    "        'name': ['Czechia', 'Netherland Antilles', 'Germany', 'European Union'], \n",
    "        'alpha-2': ['XC', 'AN', 'DD', 'EU'], \n",
    "        'region': ['Europe', 'Americas', 'Europe', 'Europe'], \n",
    "        'sub-region': \n",
    "            [\n",
    "                'Eastern Europe', \n",
    "                'Latin America and the Caribbean', \n",
    "                'Western Europe', \n",
    "                'Western Europe'\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "# Adding extra codes to countries table\n",
    "countries = pd.concat(\n",
    "    [countries, extra_codes], \n",
    "    ignore_index=True, \n",
    "    keys=['alpha-2', 'name']\n",
    ")\n",
    "\n",
    "# Change countries index column to be alpha-2 values and rename to id\n",
    "countries.set_index('alpha-2', inplace=True)\n",
    "countries.index.name = 'id'\n",
    "countries.sort_index(inplace=True)\n",
    "\n",
    "# Create country code set with O(1) lookup for table cleaning\n",
    "alpha_2_set = set(countries.index)\n",
    "\n",
    "# Create country name dictionary with O(1) lookup for table cleaning\n",
    "country_dict = {x[1].upper(): x[0] for x in countries.itertuples()}\n",
    "\n",
    "# Add some statistically siginificant outliers, including common 2 common 'typos'\n",
    "country_dict['TAIWAN'] = 'TW'\n",
    "country_dict['SOUTH KOREA'] = 'KR'\n",
    "country_dict['SHANGHAI CN'] = 'CN'\n",
    "country_dict['SHANGHAI'] = 'CN'\n",
    "country_dict['SHANGHAI .'] = 'CN'\n",
    "country_dict['HONG KONG .'] = 'CN'\n",
    "country_dict['TAIPEI .'] = 'TW'\n",
    "country_dict['USA'] = 'US'\n",
    "country_dict['U.S.A.'] = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning File Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After downloading files, I edited the file names for cleanliness and uniformity. \n",
    "- file names started as `ams_[name]_[year]__202001290000_part_[num]`\n",
    "- Simplified to: `[name]_[year]_part_[num]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read CSVs to DataFrames with only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See lib/utils.py for get_data_url function\n",
    "shipper_0 = pd.read_csv(get_data_url('bronze', 'shipper', '18', '0'))\n",
    "shipper_1 = pd.read_csv(get_data_url('bronze', 'shipper', '18', '1'))\n",
    "shipper_2 = pd.read_csv(get_data_url('bronze', 'shipper', '19', '0'))\n",
    "shipper_3 = pd.read_csv(get_data_url('bronze', 'shipper', '19', '1'))\n",
    "shipper_4 = pd.read_csv(get_data_url('bronze', 'shipper', '20', '0'))\n",
    "shipper_5 = pd.read_csv(get_data_url('bronze', 'shipper', '20', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenate shippers DataFrames to single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shippers = pd.concat(\n",
    "    [\n",
    "        shipper_0, \n",
    "        shipper_1, \n",
    "        shipper_2, \n",
    "        shipper_3, \n",
    "        shipper_4, \n",
    "        shipper_5\n",
    "    ], \n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in name with Unknown\n",
    "shippers['shipper_party_name'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean shipper rows and remove remaining unnecessary columns - (see utils.py for **CLEAN_ROW** function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_clean = shippers.apply(\n",
    "    lambda row: clean_row(row, 'shipper_party', alpha_2_set, country_dict), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Result:** out of 40,240,366 values\n",
    "\n",
    "\n",
    "|  | Before Cleaning | After Cleaning |\n",
    "| - | - | - |\n",
    "| country_codes #| 9,911,774 | 13,100,153| \n",
    "| country_codes %| 24.6% | 32.55% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create shipper id column and map IDs by name - (see utils.py for **GET_ID_NUMS** function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_id_dict = get_id_nums(shipper_clean['shipper_party_name'])\n",
    "shipper_clean['shipper_id'] = shipper_clean['shipper_party_name'].map(shipper_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write cleaned data to CSV (time consuming process -- no mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_fail_safe_path = get_data_url('fail_safe', 'shipper_clean')\n",
    "shipper_clean.to_csv(shipper_fail_safe_path, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copy CSVs to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_0 = pd.read_csv(get_data_url('bronze', 'consignee', '18', '0'))\n",
    "consignee_1 = pd.read_csv(get_data_url('bronze', 'consignee', '18', '1'))\n",
    "consignee_2 = pd.read_csv(get_data_url('bronze', 'consignee', '19', '0'))\n",
    "consignee_3 = pd.read_csv(get_data_url('bronze', 'consignee', '19', '1'))\n",
    "consignee_4 = pd.read_csv(get_data_url('bronze', 'consignee', '20', '0'))\n",
    "consignee_5 = pd.read_csv(get_data_url('bronze', 'consignee', '20', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenate to single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignees = pd.concat(\n",
    "    [\n",
    "        consignee_0, \n",
    "        consignee_1, \n",
    "        consignee_2, \n",
    "        consignee_3, \n",
    "        consignee_4, \n",
    "        consignee_5\n",
    "    ], \n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill NaN names with N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignees['consignee_name'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean names and country codes, remove unneceessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_clean = consignees.apply(\n",
    "    lambda row: clean_row(row, 'consignee', alpha_2_set, country_dict), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Map IDs to consignees by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_id_dict = get_id_nums(consignee_clean['consignee_name'])\n",
    "consignee_clean['consignee_id'] = consignee_clean['consignee_name'].map(consignee_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save expensive task to CSV in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_fail_safe_path = get_data_url('fail_safe', 'consignee_clean')\n",
    "consignee_clean.to_csv(consignee_fail_safe_path, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read header CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_0 = pd.read_csv(get_data_url('bronze', 'header', '18', '0'))\n",
    "header_1 = pd.read_csv(get_data_url('bronze', 'header', '18', '1'))\n",
    "header_2 = pd.read_csv(get_data_url('bronze', 'header', '18', '2'))\n",
    "header_3 = pd.read_csv(get_data_url('bronze', 'header', '18', '3'))\n",
    "header_4 = pd.read_csv(get_data_url('bronze', 'header', '19', '0'))\n",
    "header_5 = pd.read_csv(get_data_url('bronze', 'header', '19', '1'))\n",
    "header_6 = pd.read_csv(get_data_url('bronze', 'header', '19', '2'))\n",
    "header_7 = pd.read_csv(get_data_url('bronze', 'header', '19', '3'))\n",
    "header_8 = pd.read_csv(get_data_url('bronze', 'header', '20', '0'))\n",
    "header_9 = pd.read_csv(get_data_url('bronze', 'header', '20', '1'))\n",
    "header_10 = pd.read_csv(get_data_url('bronze', 'header', '20', '2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concat to a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = pd.concat(\n",
    "    [\n",
    "        header_0, \n",
    "        header_1, \n",
    "        header_2, \n",
    "        header_3, \n",
    "        header_4, \n",
    "        header_5, \n",
    "        header_6, \n",
    "        header_7, \n",
    "        header_8, \n",
    "        header_9, \n",
    "        header_10], \n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vessel Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate out vessel table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_silver_cols = [\n",
    "    'vessel_name', \n",
    "    'vessel_country_code', \n",
    "    'carrier_code', \n",
    "    'conveyance_id_qualifier', \n",
    "    'conveyance_id'\n",
    "]\n",
    "vessels = headers[['identifier'] + vessel_silver_cols]\n",
    "\n",
    "vessels.loc[:, 'vessel_name'] = vessels['vessel_name'].fillna('N/A')\n",
    "vessels.loc[:, 'vessel_country_code'] = vessels['vessel_country_code'].fillna('N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get unique vessels from name + country and assign ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels['temp'] = vessels['vessel_name'] + ' ' + vessels['vessel_country_code']\n",
    "vessel_id_dict = get_id_nums(vessels['temp'])\n",
    "vessels['vessel_id'] = vessels['temp'].map(vessel_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge with header on identifier to assign vessel_id as foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = headers.merge(\n",
    "    vessels[['identifier', 'vessel_id']], \n",
    "    on=['identifier'], \n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove repeat vessels, drop identifier and temp column, set index as id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.drop_duplicates(subset=['temp'], inplace=True)\n",
    "vessels.drop(labels=['identifier', 'temp'], axis=1, inplace=True)\n",
    "vessels.set_index(keys='vessel_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove erroneous or unknown country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels['vessel_country_code'] = vessels['vessel_country_code'].apply(\n",
    "    lambda row: remove_incorrect_codes(row, alpha_2_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove vessel columns and columns with non-nulls <= 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify = 'secondary_notify_party_'\n",
    "\n",
    "insig_cols = [\n",
    "        'port_of_destination', \n",
    "        'foreign_port_of_destination', \n",
    "        'foreign_port_of_destination_qualifier', \n",
    "        'in_bond_entry_type', \n",
    "        notify + '2', notify + '3', notify + '4', \n",
    "        notify + '5', notify + '6', notify + '7', \n",
    "        notify + '8', notify + '9', notify + '10'\n",
    "    ]\n",
    "\n",
    "headers.drop(vessel_silver_cols + insig_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junction Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_rename = {'identifier': 'shipment_id'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shipper_Shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_shipment = shipper_clean[['identifier', 'shipper_id']].copy()\n",
    "shipper_shipment.index.name = 'shipper_shipment_id'\n",
    "shipper_shipment.rename(columns=identifier_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consignee_Shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_shipment = consignee_clean[['identifier', 'consignee_id']].copy()\n",
    "consignee_shipment.index.name = 'cosignee_shipment_id'\n",
    "consignee_shipment.rename(columns=identifier_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](silver_erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove identifier from consignee and shipper, drop duplicates on id, change index to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_clean.drop(['identifier'], axis=1, inplace=True)\n",
    "shipper_clean.drop_duplicates(subset=['shipper_id'], inplace=True)\n",
    "shipper_clean.set_index(['shipper_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_clean.drop(['identifier'], axis=1, inplace=True)\n",
    "consignee_clean.drop_duplicates(subset=['consignee_id'], inplace=True)\n",
    "consignee_clean.set_index(['consignee_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change index on header to identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers.set_index(['identifier'], inplace=True)\n",
    "headers.index.name = 'shipment_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upload shipper, shipment, shipper_shipment, consignee_shipment, consignee as as csv for SQL batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers.to_csv(get_data_url('silver', 'shipment'), mode='w')\n",
    "vessels.to_csv(get_data_url('silver', 'vessel'), mode='w')\n",
    "shipper_clean.to_csv(get_data_url('silver', 'shipper'), mode='w')\n",
    "consignee_clean.to_csv(get_data_url('silver', 'consignee'), mode='w')\n",
    "shipper_shipment.to_csv(get_data_url('silver', 'shipper_shipment'), mode='w')\n",
    "consignee_shipment.to_csv(get_data_url('silver', 'consignee_shipment'), mode='w')\n",
    "countries.to_csv(get_data_url('silver', 'country'), mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ERD](erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prep tables for Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_clean.rename(columns={\"shipper_party_name\": \"shipper_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns to keep\n",
    "shipper_cols = ['shipper_party_name', 'country_code']\n",
    "consignee_cols = ['consignee_name', 'country_code']\n",
    "header_cols = ['vessel_id', 'estimated_arrival_date','actual_arrival_date']\n",
    "vessel_cols = ['vessel_name', 'vessel_country_code', 'carrier_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upload 'denormalized' gold layer tables to folder for virtual mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_csv(get_data_url('gold', 'country'), mode='w')\n",
    "vessels[vessel_cols].to_csv(get_data_url('gold', 'vessel'), mode='w')\n",
    "headers[header_cols].to_csv(get_data_url('gold', 'shipment'), mode='w')\n",
    "shipper_clean[shipper_cols].to_csv(get_data_url('gold', 'shipper'), mode='w')\n",
    "consignee_clean[consignee_cols].to_csv(get_data_url('gold', 'consignee'), mode='w')\n",
    "shipper_shipment.to_csv(get_data_url('gold', 'shipper_shipment'), mode='w')\n",
    "consignee_shipment.to_csv(get_data_url('gold', 'consignee_shipment'), mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late Emendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After everything, and only during SQL querying, I noticed an overlooked error -- dates outside the range of 2017-2020 which caused arrival deltas of between 300 and 3000 days.\n",
    "- Late into the project, with time not on my side, I decided to fix it at the gold layer - as crunching the data from the silver layer would take too much time\n",
    "- If I had more time, I would have added this cleaning step to the silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My fix was to drop all dates outside the specified years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_fix = pd.read_csv(\n",
    "    get_data_url('gold', 'shipment'), \n",
    "    parse_dates=['estimated_arrival_date', 'actual_arrival_date'], \n",
    "    index_col=[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_fixed = sh_fix[\n",
    "    (sh_fix['estimated_arrival_date'].dt.year.isin([2018, 2019, 2020])) & \n",
    "    (sh_fix['actual_arrival_date'].dt.year.isin([2018, 2019, 2020]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_fixed.to_csv(get_data_url('gold', 'shipment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = os.getenv('SERVER')\n",
    "username = os.getenv('USER')\n",
    "password = os.getenv('PASSWORD')\n",
    "driver = os.getenv('DRIVER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n",
      "\n",
      "USE statement executed\n",
      "CREATE DATABASE statement executed\n",
      "USE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "Connection closed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_tables_file = 'sql/create_db_tables.sql'\n",
    "\n",
    "try: \n",
    "    conn = pyodbc.connect(\n",
    "        driver=driver, \n",
    "        server=server, \n",
    "        uid=username, \n",
    "        pwd=password, \n",
    "        autocommit=True\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    print(\"Connected Successfully!\\n\")\n",
    "    \n",
    "    with open(create_tables_file, 'r') as f:\n",
    "        # Split statements by full statement (ending with ';')\n",
    "        sql_statements = f.read().replace('\\n', '').strip().split(\";\")\n",
    "        # Add semi-colon back as long as the statement is not empty\n",
    "        sql_statements = [x + ';' for x in sql_statements if x]\n",
    "\n",
    "        for statement in sql_statements:\n",
    "            cursor.execute(statement)\n",
    "\n",
    "            # Console.log statement to check progress in case of errors\n",
    "            split = statement.split('    ')\n",
    "            if split[0].split()[0] == 'USE':\n",
    "                cmd_run = split[0].split()[0]\n",
    "            else:\n",
    "                cmd_run = f\"{split[1].split()[0]} {split[1].split()[1]}\"\n",
    "\n",
    "            print(f\"{cmd_run} statement executed\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('Connection closed\\n')\n",
    "    else:\n",
    "        print('No connection was established')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulk Insert from Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n",
      "\n",
      "USE ams  statement executed\n",
      "BULK INSERT dbo.country statement executed\n",
      "BULK INSERT dbo.vessel statement executed\n",
      "BULK INSERT dbo.shipment statement executed\n",
      "BULK INSERT dbo.shipper statement executed\n",
      "BULK INSERT dbo.consignee statement executed\n",
      "BULK INSERT dbo.shipper_shipment statement executed\n",
      "BULK INSERT dbo.consignee_shipment statement executed\n",
      "\n",
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "bulk_load_file = 'sql/load_db_tables.sql'\n",
    "\n",
    "try: \n",
    "    conn = pyodbc.connect(\n",
    "        driver=driver, \n",
    "        server=server, \n",
    "        uid=username, \n",
    "        pwd=password, \n",
    "        autocommit=True\n",
    "    )\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(\"Connected Successfully!\\n\")\n",
    "\n",
    "    with open(bulk_load_file, 'r') as f:\n",
    "        sql_statements = f.read().replace('\\n', '').split(\";\")\n",
    "        sql_statements = [x for x in sql_statements if x]\n",
    "\n",
    "        for statement in sql_statements:\n",
    "            cursor.execute(statement)\n",
    "            \n",
    "            # Console.log statement to check progress in case of errors\n",
    "            words = statement.split()\n",
    "            print(\n",
    "                f\"{words[0]} {words[1]} {words[2] if len(words) > 2 else ''} statement executed\"\n",
    "            )\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('\\nConnection closed')\n",
    "    else:\n",
    "        print('\\nNo connection was established')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT statements vars\n",
    "avg_delta = 'AVG(t.delta) AS avg_delta'\n",
    "total_shipments = 'COUNT(t.delta) AS total_shipments'\n",
    "delta = 'CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta'\n",
    "\n",
    "# JOIN statement vars\n",
    "shipper_shipment_to_shipment = 'shipper_shipment ss ON sh.shipment_id = ss.shipment_id'\n",
    "shipper_to_shipper_shipment = 'shipper s ON ss.shipper_id = s.shipper_id'\n",
    "country_to_shipper = 'country c ON s.shipper_country_code = c.country_id'\n",
    "vessel_to_shipment = 'vessel v ON sh.vessel_id = v.vessel_id'\n",
    "\n",
    "# WHERE statement vars\n",
    "years_equal = '(YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date))'\n",
    "delta_within_range = 'ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60'\n",
    "min_shipments_met = 'delta_t.total_shipments > 100'\n",
    "\n",
    "# Find top shippers\n",
    "statement_1 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.shipper_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT s.shipper_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {shipper_shipment_to_shipment}\n",
    "            JOIN {shipper_to_shipper_shipment}\n",
    "            WHERE {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.shipper_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "# Find bottom shippers\n",
    "statement_2 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.shipper_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT s.shipper_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {shipper_shipment_to_shipment}\n",
    "            JOIN {shipper_to_shipper_shipment}\n",
    "            WHERE {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.shipper_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta DESC\n",
    "'''\n",
    "\n",
    "# Find top countries\n",
    "statement_3 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.country_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT c.country_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {shipper_shipment_to_shipment}\n",
    "            JOIN {shipper_to_shipper_shipment}\n",
    "            JOIN {country_to_shipper}\n",
    "            WHERE s.shipper_country_code IS NOT NULL AND {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.country_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "# Find bottom countries\n",
    "statement_4 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.country_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT c.country_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {shipper_shipment_to_shipment}\n",
    "            JOIN {shipper_to_shipper_shipment}\n",
    "            JOIN {country_to_shipper}\n",
    "            WHERE s.shipper_country_code IS NOT NULL AND {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.country_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta DESC\n",
    "'''\n",
    "\n",
    "# Find all regions and rank\n",
    "statement_5 = f'''\n",
    "SELECT * FROM \n",
    "(\n",
    "    SELECT t.region, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT c.region, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {shipper_shipment_to_shipment}\n",
    "            JOIN {shipper_to_shipper_shipment}\n",
    "            JOIN {country_to_shipper}\n",
    "            WHERE \n",
    "                s.shipper_country_code IS NOT NULL\n",
    "                AND c.region IS NOT NULL\n",
    "                AND {years_equal} \n",
    "                AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.region\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "# Find all sub-regions and rank\n",
    "statement_6 = f'''\n",
    "SELECT * FROM \n",
    "(\n",
    "    SELECT t.sub_region, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT c.sub_region, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {shipper_shipment_to_shipment}\n",
    "            JOIN {shipper_to_shipper_shipment}\n",
    "            JOIN {country_to_shipper}\n",
    "            WHERE \n",
    "                s.shipper_country_code IS NOT NULL\n",
    "                AND c.sub_region IS NOT NULL\n",
    "                AND {years_equal} \n",
    "                AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.sub_region\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "# Find top vessels\n",
    "statement_7 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.vessel_id, t.vessel_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT v.vessel_id, v.vessel_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {vessel_to_shipment}\n",
    "            WHERE {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.vessel_id, t.vessel_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "# Find bottom vessels\n",
    "statement_8 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.vessel_id, t.vessel_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT v.vessel_id, v.vessel_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN {vessel_to_shipment}\n",
    "            WHERE {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.vessel_id, t.vessel_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta DESC\n",
    "'''\n",
    "\n",
    "# Find averages per year\n",
    "statement_9 = f'''\n",
    "SELECT * FROM \n",
    "(\n",
    "    SELECT t.year, {avg_delta}, {total_shipments}s\n",
    "    FROM \n",
    "        (\n",
    "            SELECT YEAR(sh.actual_arrival_date) AS year, {delta}\n",
    "            FROM shipment sh\n",
    "            WHERE {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.year\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.year ASC\n",
    "'''\n",
    "\n",
    "# Find consginees who chose shippers best\n",
    "statement_10 = f'''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.consignee_name, {avg_delta}, {total_shipments}\n",
    "    FROM \n",
    "        (\n",
    "            SELECT c.consignee_name, {delta}\n",
    "            FROM shipment sh\n",
    "            JOIN consignee_shipment cs ON cs.shipment_id = sh.shipment_id\n",
    "            JOIN consignee c ON c.consignee_id = cs.consignee_id\n",
    "            WHERE \n",
    "                {years_equal} AND {delta_within_range}\n",
    "        ) AS t\n",
    "    GROUP BY t.consignee_name\n",
    ") AS delta_t\n",
    "WHERE {min_shipments_met}\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    conn = pyodbc.connect(\n",
    "        driver=driver, \n",
    "        server=server, \n",
    "        uid=username, \n",
    "        pwd=password, \n",
    "        database='ams', \n",
    "        autocommit=True\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    print(\"Connected Successfully!\\n\")\n",
    "\n",
    "    top_shippers = pd.read_sql(statement_1, conn)\n",
    "    bottom_shippers = pd.read_sql(statement_2, conn)\n",
    "    top_countries = pd.read_sql(statement_3, conn)\n",
    "    bottom_countries = pd.read_sql(statement_4, conn)\n",
    "    regions_ranked = pd.read_sql(statement_5, conn)\n",
    "    sub_regions_ranked = pd.read_sql(statement_6, conn)\n",
    "    top_vessels = pd.read_sql(statement_7, conn)\n",
    "    bottom_vessels = pd.read_sql(statement_8, conn)\n",
    "    years = pd.read_sql(statement_9, conn)\n",
    "    most_savvy_consignees = pd.read_sql(statement_10, conn)\n",
    "    \n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('\\nConnection closed\\n')\n",
    "    else:\n",
    "        print('\\nNo connection was established\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Results - Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've included images of the resulting returned SQL queries as DataFrames so you don't have to run this whole thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 1: Shippers with the best estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_shippers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 2: Shippers with the worst estimated vs actual times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_shippers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 3: Countries with the best estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 4: Countries with the worst estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 5: Regions ranked by estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 6: Sub-regions ranked by estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_regions_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 7: Vessels with the best estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vessels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 8: Vessels with the worst estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_vessels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 9: Years with average estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view09.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View 10: Consignees with the best estimated vs actual arrival times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_savvy_consignees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](results_imgs/view10.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
