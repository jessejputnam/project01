{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big Question\n",
    "- Which shippers are most/least reliable (arrival time delta between estimated and actual)?\n",
    "\n",
    "#### Sub Questions\n",
    "- Which are the most reliable shippers per country/region/subregion\n",
    "- Which carrier companies are the most reliable?\n",
    "- What, if any, were the reliability changes over the years?\n",
    "    - How did covid affect reliability metrics of shipment times?\n",
    "- Which consignees chose their shippers wisest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lib.utils import get_id_nums, clean_row, remove_incorrect_codes, data_fetch_url, data_send_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up countries table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get table of countries with alpha-2 code that includes region from repository\n",
    "    - https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_cols = ['name', 'alpha-2', 'region', 'sub-region']\n",
    "# keep_default_na=False because Namibia's code is NA\n",
    "countries = pd.read_csv(data_send_url('bronze', 'country_data'), usecols=countries_cols, keep_default_na=False)\n",
    "\n",
    "extra_codes = pd.DataFrame(\n",
    "    {'name': ['Czechia', 'Netherland Antilles', 'Germany', 'European Union'], \n",
    "    'alpha-2': ['XC', 'AN', 'DD', 'EU'], \n",
    "    'region': ['Europe', 'Americas', 'Europe', 'Europe'], \n",
    "    'sub-region': ['Eastern Europe', 'Latin America and the Caribbean', 'Western Europe', 'Western Europe']}\n",
    "    )\n",
    "\n",
    "extra_codes\n",
    "countries = pd.concat([countries, extra_codes], ignore_index=True, keys=['alpha-2', 'name'])\n",
    "\n",
    "# Change countries index column to be alpha-2 values and rename to id\n",
    "countries.set_index('alpha-2', inplace=True)\n",
    "countries.index.name = 'id'\n",
    "countries.sort_index(inplace=True)\n",
    "\n",
    "# Create country code set with O(1) lookup for table cleaning\n",
    "alpha_2_set = set(countries.index)\n",
    "\n",
    "# Create country name dictionary with O(1) lookup for table cleaning\n",
    "country_dict = {x[1].upper(): x[0] for x in countries.itertuples()}\n",
    "\n",
    "# Add some statistically siginificant outliers, including common 2 common 'typos'\n",
    "country_dict['TAIWAN'] = 'TW'\n",
    "country_dict['SOUTH KOREA'] = 'KR'\n",
    "country_dict['SHANGHAI CN'] = 'CN'\n",
    "country_dict['SHANGHAI'] = 'CN'\n",
    "country_dict['SHANGHAI .'] = 'CN'\n",
    "country_dict['HONG KONG .'] = 'CN'\n",
    "country_dict['TAIPEI .'] = 'TW'\n",
    "country_dict['USA'] = 'US'\n",
    "country_dict['U.S.A.'] = 'US'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read CSVs to DataFrames with only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_0 = pd.read_csv(data_fetch_url('bronze', 'shipper', '18', '0'))\n",
    "shipper_1 = pd.read_csv(data_fetch_url('bronze', 'shipper', '18', '1'))\n",
    "shipper_2 = pd.read_csv(data_fetch_url('bronze', 'shipper', '19', '0'))\n",
    "shipper_3 = pd.read_csv(data_fetch_url('bronze', 'shipper', '19', '1'))\n",
    "shipper_4 = pd.read_csv(data_fetch_url('bronze', 'shipper', '20', '0'))\n",
    "shipper_5 = pd.read_csv(data_fetch_url('bronze', 'shipper', '20', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenate shippers DataFrames to single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shippers = pd.concat([shipper_0, shipper_1, shipper_2, shipper_3, shipper_4, shipper_5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in name with Unknown\n",
    "shippers['shipper_party_name'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean shipper rows and remove remaining unnecessary columns - (see utils.py for **CLEAN_ROW** function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_clean = shippers.apply(lambda row: clean_row(row, 'shipper_party', alpha_2_set, country_dict), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Result:** out of 40,240,366 values\n",
    "\n",
    "\n",
    "|  | Before Cleaning | After Cleaning |\n",
    "| - | - | - |\n",
    "| country_codes #| 9,911,774 | 13,100,153| \n",
    "| country_codes %| 24.6% | 32.55% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create shipper id column and map IDs by name - (see utils.py for **GET_ID_NUMS** function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_id_dict = get_id_nums(shipper_clean['shipper_party_name'])\n",
    "shipper_clean['shipper_id'] = shipper_clean['shipper_party_name'].map(shipper_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write cleaned data to CSV (time consuming process -- no mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_fail_safe_path = data_send_url('fail_safe', 'shipper_clean')\n",
    "shipper_clean.to_csv(shipper_fail_safe_path, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consignee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copy CSVs to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_0 = pd.read_csv(data_fetch_url('bronze', 'consignee', '18', '0'))\n",
    "consignee_1 = pd.read_csv(data_fetch_url('bronze', 'consignee', '18', '1'))\n",
    "consignee_2 = pd.read_csv(data_fetch_url('bronze', 'consignee', '19', '0'))\n",
    "consignee_3 = pd.read_csv(data_fetch_url('bronze', 'consignee', '19', '1'))\n",
    "consignee_4 = pd.read_csv(data_fetch_url('bronze', 'consignee', '20', '0'))\n",
    "consignee_5 = pd.read_csv(data_fetch_url('bronze', 'consignee', '20', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenate to single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignees = pd.concat([consignee_0, consignee_1, consignee_2, consignee_3, consignee_4, consignee_5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fill NaN names with N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignees['consignee_name'].fillna('N/A', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean names and country codes, remove unneceessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_clean = consignees.apply(lambda row: clean_row(row, 'consignee', alpha_2_set, country_dict), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Map IDs to consignees by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_id_dict = get_id_nums(consignee_clean['consignee_name'])\n",
    "consignee_clean['consignee_id'] = consignee_clean['consignee_name'].map(consignee_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save expensive task to CSV in case of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_fail_safe_path = data_send_url('fail_safe', 'consignee_clean')\n",
    "consignee_clean.to_csv(consignee_fail_safe_path, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read header CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_0 = pd.read_csv(data_fetch_url('bronze', 'header', '18', '0'))\n",
    "header_1 = pd.read_csv(data_fetch_url('bronze', 'header', '18', '1'))\n",
    "header_2 = pd.read_csv(data_fetch_url('bronze', 'header', '18', '2'))\n",
    "header_3 = pd.read_csv(data_fetch_url('bronze', 'header', '18', '3'))\n",
    "header_4 = pd.read_csv(data_fetch_url('bronze', 'header', '19', '0'))\n",
    "header_5 = pd.read_csv(data_fetch_url('bronze', 'header', '19', '1'))\n",
    "header_6 = pd.read_csv(data_fetch_url('bronze', 'header', '19', '2'))\n",
    "header_7 = pd.read_csv(data_fetch_url('bronze', 'header', '19', '3'))\n",
    "header_8 = pd.read_csv(data_fetch_url('bronze', 'header', '20', '0'))\n",
    "header_9 = pd.read_csv(data_fetch_url('bronze', 'header', '20', '1'))\n",
    "header_10 = pd.read_csv(data_fetch_url('bronze', 'header', '20', '2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concat to a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = pd.concat([header_0, header_1, header_2, header_3, header_4, header_5, header_6, header_7, header_8, header_9, header_10], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vessel Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate out vessel table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_silver_cols = ['vessel_name', 'vessel_country_code', 'carrier_code', 'conveyance_id_qualifier', 'conveyance_id']\n",
    "vessels = headers[['identifier'] + vessel_silver_cols]\n",
    "vessels.loc[:, 'vessel_name'] = vessels['vessel_name'].fillna('N/A')\n",
    "vessels.loc[:, 'vessel_country_code'] = vessels['vessel_country_code'].fillna('N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get unique vessels from name + country and assign ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels['temp'] = vessels['vessel_name'] + ' ' + vessels['vessel_country_code']\n",
    "vessel_id_dict = get_id_nums(vessels['temp'])\n",
    "vessels['vessel_id'] = vessels['temp'].map(vessel_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge with header on identifier to assign vessel_id as foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = headers.merge(vessels[['identifier', 'vessel_id']], on=['identifier'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove repeat vessels, drop identifier and temp column, set index as id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.drop_duplicates(subset=['temp'], inplace=True)\n",
    "vessels.drop(labels=['identifier', 'temp'], axis=1, inplace=True)\n",
    "vessels.set_index(keys='vessel_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove erroneous or unknown country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels['vessel_country_code'] = vessels['vessel_country_code'].apply(lambda row: remove_incorrect_codes(row, alpha_2_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove vessel columns and columns with non-nulls <= 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "notify = 'secondary_notify_party_'\n",
    "insig_cols = [\n",
    "        'port_of_destination', \n",
    "        'foreign_port_of_destination', \n",
    "        'foreign_port_of_destination_qualifier', \n",
    "        'in_bond_entry_type', \n",
    "        notify + '2', notify + '3', notify + '4', notify + '5', \n",
    "        notify + '6', notify + '7', notify + '8', notify + '9', notify + '10'\n",
    "    ]\n",
    "\n",
    "headers.drop(vessel_silver_cols + insig_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Junction Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_rename = {'identifier': 'shipment_id'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shipper_Shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_shipment = shipper_clean[['identifier', 'shipper_id']].copy()\n",
    "shipper_shipment.index.name = 'shipper_shipment_id'\n",
    "shipper_shipment.rename(columns=identifier_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consignee_Shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_shipment = consignee_clean[['identifier', 'consignee_id']].copy()\n",
    "consignee_shipment.index.name = 'cosignee_shipment_id'\n",
    "consignee_shipment.rename(columns=identifier_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](/Users/jesseputnam/cs-learning/skillstorm/project01/silver_erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove identifier from consignee and shipper, drop duplicates on id, change index to IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_clean.drop(['identifier'], axis=1, inplace=True)\n",
    "shipper_clean.drop_duplicates(subset=['shipper_id'], inplace=True)\n",
    "shipper_clean.set_index(['shipper_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "consignee_clean.drop(['identifier'], axis=1, inplace=True)\n",
    "consignee_clean.drop_duplicates(subset=['consignee_id'], inplace=True)\n",
    "consignee_clean.set_index(['consignee_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change index on header to identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers.set_index(['identifier'], inplace=True)\n",
    "headers.index.name = 'shipment_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upload shipper, shipment, shipper_shipment, consignee_shipment, consignee as as csv for SQL batch loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers.to_csv(data_send_url('silver', 'shipment'), mode='w')\n",
    "vessels.to_csv(data_send_url('silver', 'vessel'), mode='w')\n",
    "shipper_clean.to_csv(data_send_url('silver', 'shipper'), mode='w')\n",
    "consignee_clean.to_csv(data_send_url('silver', 'consignee'), mode='w')\n",
    "shipper_shipment.to_csv(data_send_url('silver', 'shipper_shipment'), mode='w')\n",
    "consignee_shipment.to_csv(data_send_url('silver', 'consignee_shipment'), mode='w')\n",
    "countries.to_csv(data_send_url('silver', 'country'), mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ERD](/Users/jesseputnam/cs-learning/skillstorm/project01/erd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prep tables for Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipper_clean.rename(columns={\"shipper_party_name\": \"shipper_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns to keep\n",
    "shipper_cols = ['shipper_party_name', 'country_code']\n",
    "consignee_cols = ['consignee_name', 'country_code']\n",
    "header_cols = ['vessel_id', 'estimated_arrival_date','actual_arrival_date']\n",
    "vessel_cols = ['vessel_name', 'vessel_country_code', 'carrier_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upload 'denormalized' gold layer tables to folder for virtual mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.to_csv(data_send_url('gold', 'country'), mode='w')\n",
    "vessels[vessel_cols].to_csv(data_send_url('gold', 'vessel'), mode='w')\n",
    "headers[header_cols].to_csv(data_send_url('gold', 'shipment'), mode='w')\n",
    "shipper_clean[shipper_cols].to_csv(data_send_url('gold', 'shipper'), mode='w')\n",
    "consignee_clean[consignee_cols].to_csv(data_send_url('gold', 'consignee'), mode='w')\n",
    "shipper_shipment.to_csv(data_send_url('gold', 'shipper_shipment'), mode='w')\n",
    "consignee_shipment.to_csv(data_send_url('gold', 'consignee_shipment'), mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Late Emendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After everything, and only during SQL querying, I noticed an overlooked error -- dates outside the range of 2017-2020 which caused arrival deltas of between 300 and 3000 days.\n",
    "- Late into the project, with time not on my side, I decided to fix it at the gold layer - as crunching the data from the silver layer would take too much time\n",
    "- If I had more time, I would have added this cleaning step to the silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My fix was to drop all dates outside the specified years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_fix = pd.read_csv(data_send_url('gold', 'shipment'), parse_dates=['estimated_arrival_date', 'actual_arrival_date'], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_fixed = sh_fix[(sh_fix['estimated_arrival_date'].dt.year.isin([2018, 2019, 2020])) & (sh_fix['actual_arrival_date'].dt.year.isin([2018, 2019, 2020]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_fixed.to_csv(data_send_url('gold', 'shipment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = os.getenv('SERVER')\n",
    "username = os.getenv('USER')\n",
    "password = os.getenv('PASSWORD')\n",
    "driver = os.getenv('DRIVER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n",
      "\n",
      "USE statement executed\n",
      "CREATE DATABASE statement executed\n",
      "USE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "CREATE TABLE statement executed\n",
      "Connection closed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_tables_file = '/Users/jesseputnam/cs-learning/skillstorm/project01/sql/create_db_tables.sql'\n",
    "\n",
    "try: \n",
    "    conn = pyodbc.connect(driver=driver, server=server, uid=username, pwd=password, autocommit=True)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected Successfully!\\n\")\n",
    "    \n",
    "    with open(create_tables_file, 'r') as f:\n",
    "        sql_statements = f.read().replace('\\n', '').strip().split(\";\")\n",
    "        sql_statements = [x + ';' for x in sql_statements if x]\n",
    "\n",
    "        for statement in sql_statements:\n",
    "            cursor.execute(statement)\n",
    "\n",
    "            split = statement.split('    ')\n",
    "            if split[0].split()[0] == 'USE':\n",
    "                cmd_run = split[0].split()[0]\n",
    "            else:\n",
    "                cmd_run = f\"{split[1].split()[0]} {split[1].split()[1]}\"\n",
    "\n",
    "            print(f\"{cmd_run} statement executed\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('Connection closed\\n')\n",
    "    else:\n",
    "        print('No connection was established')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulk Insert from Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n",
      "\n",
      "USE ams  statement executed\n",
      "BULK INSERT dbo.country statement executed\n",
      "BULK INSERT dbo.vessel statement executed\n",
      "BULK INSERT dbo.shipment statement executed\n",
      "BULK INSERT dbo.shipper statement executed\n",
      "BULK INSERT dbo.consignee statement executed\n",
      "BULK INSERT dbo.shipper_shipment statement executed\n",
      "BULK INSERT dbo.consignee_shipment statement executed\n",
      "\n",
      "Connection closed\n"
     ]
    }
   ],
   "source": [
    "bulk_load_file = '/Users/jesseputnam/cs-learning/skillstorm/project01/sql/load_db_tables.sql'\n",
    "\n",
    "try: \n",
    "    conn = pyodbc.connect(driver=driver, server=server, uid=username, pwd=password, autocommit=True)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected Successfully!\\n\")\n",
    "\n",
    "    with open(bulk_load_file, 'r') as f:\n",
    "        sql_statements = f.read().replace('\\n', '').split(\";\")\n",
    "        sql_statements = [x for x in sql_statements if x]\n",
    "\n",
    "        for statement in sql_statements:\n",
    "            cursor.execute(statement)\n",
    "            words = statement.split()\n",
    "            print(f\"{words[0]} {words[1]} {words[2] if len(words) > 2 else ''} statement executed\")\n",
    "\n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('\\nConnection closed')\n",
    "    else:\n",
    "        print('\\nNo connection was established')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement_1 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.shipper_name, AVG(t.delta) AS avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                s.shipper_name, \n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN shipper_shipment ss ON sh.shipment_id = ss.shipment_id\n",
    "            JOIN shipper s ON ss.shipper_id = s.shipper_id\n",
    "            WHERE \n",
    "                (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.shipper_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "statement_2 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.shipper_name, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                s.shipper_name, \n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN shipper_shipment ss ON sh.shipment_id = ss.shipment_id\n",
    "            JOIN shipper s ON ss.shipper_id = s.shipper_id\n",
    "            WHERE \n",
    "                (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.shipper_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta DESC\n",
    "'''\n",
    "\n",
    "statement_3 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.country_name, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                c.country_name,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN shipper_shipment ss ON sh.shipment_id = ss.shipment_id\n",
    "            JOIN shipper s ON ss.shipper_id = s.shipper_id\n",
    "            JOIN country c ON s.shipper_country_code = c.country_id\n",
    "            WHERE \n",
    "                s.shipper_country_code IS NOT NULL\n",
    "                AND (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.country_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "statement_4 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.country_name, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                c.country_name,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN shipper_shipment ss ON sh.shipment_id = ss.shipment_id\n",
    "            JOIN shipper s ON ss.shipper_id = s.shipper_id\n",
    "            JOIN country c ON s.shipper_country_code = c.country_id\n",
    "            WHERE \n",
    "                s.shipper_country_code IS NOT NULL\n",
    "                AND (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.country_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta DESC\n",
    "'''\n",
    "\n",
    "statement_5 = '''\n",
    "SELECT * FROM \n",
    "(\n",
    "    SELECT t.region, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                c.region,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN shipper_shipment ss ON sh.shipment_id = ss.shipment_id\n",
    "            JOIN shipper s ON ss.shipper_id = s.shipper_id\n",
    "            JOIN country c ON s.shipper_country_code = c.country_id\n",
    "            WHERE \n",
    "                s.shipper_country_code IS NOT NULL\n",
    "                AND c.region IS NOT NULL\n",
    "                AND (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.region\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "statement_6 = '''\n",
    "SELECT * FROM \n",
    "(\n",
    "    SELECT t.sub_region, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                c.sub_region,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN shipper_shipment ss ON sh.shipment_id = ss.shipment_id\n",
    "            JOIN shipper s ON ss.shipper_id = s.shipper_id\n",
    "            JOIN country c ON s.shipper_country_code = c.country_id\n",
    "            WHERE \n",
    "                s.shipper_country_code IS NOT NULL\n",
    "                AND c.sub_region IS NOT NULL\n",
    "                AND (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.sub_region\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "statement_7 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.vessel_id, t.vessel_name, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                v.vessel_id,\n",
    "                v.vessel_name,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN vessel v ON sh.vessel_id = v.vessel_id\n",
    "            WHERE \n",
    "                (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.vessel_id, t.vessel_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''\n",
    "\n",
    "statement_8 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.vessel_id, t.vessel_name, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                v.vessel_id,\n",
    "                v.vessel_name,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN vessel v ON sh.vessel_id = v.vessel_id\n",
    "            WHERE \n",
    "                (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.vessel_id, t.vessel_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta DESC\n",
    "'''\n",
    "\n",
    "statement_9 = '''\n",
    "SELECT * FROM \n",
    "(\n",
    "    SELECT t.year, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                YEAR(sh.actual_arrival_date) AS year,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            WHERE \n",
    "                (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.year\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.year ASC\n",
    "'''\n",
    "\n",
    "statement_10 = '''\n",
    "SELECT TOP 15 * FROM \n",
    "(\n",
    "    SELECT t.consignee_name, AVG(t.delta) as avg_delta, COUNT(t.delta) AS total_shipments\n",
    "    FROM \n",
    "        (\n",
    "            SELECT \n",
    "                c.consignee_name,\n",
    "                CAST(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date) AS FLOAT) AS delta\n",
    "            FROM shipment sh\n",
    "            JOIN consignee_shipment cs ON cs.shipment_id = sh.shipment_id\n",
    "            JOIN consignee c ON c.consignee_id = cs.consignee_id\n",
    "            WHERE \n",
    "                (YEAR(sh.actual_arrival_date) = YEAR(sh.estimated_arrival_date)) \n",
    "                AND ABS(DATEDIFF(day, sh.estimated_arrival_date, sh.actual_arrival_date)) < 60\n",
    "        ) AS t\n",
    "    GROUP BY t.consignee_name\n",
    ") AS delta_t\n",
    "WHERE delta_t.total_shipments > 100\n",
    "ORDER BY delta_t.avg_delta ASC\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected Successfully!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/3qtjthpj3qqf5dzr_n4g__mw0000gn/T/ipykernel_63685/3333904069.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  most_savvy_consignees = pd.read_sql(statement_10, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection closed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    conn = pyodbc.connect(driver=driver, server=server, uid=username, pwd=password, database='ams', autocommit=True)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"Connected Successfully!\\n\")\n",
    "\n",
    "    top_shippers = pd.read_sql(statement_1, conn)\n",
    "    bottom_shippers = pd.read_sql(statement_2, conn)\n",
    "    top_countries = pd.read_sql(statement_3, conn)\n",
    "    bottom_countries = pd.read_sql(statement_4, conn)\n",
    "    regions_ranked = pd.read_sql(statement_5, conn)\n",
    "    sub_regions_ranked = pd.read_sql(statement_6, conn)\n",
    "    top_vessels = pd.read_sql(statement_7, conn)\n",
    "    bottom_vessels = pd.read_sql(statement_8, conn)\n",
    "    years = pd.read_sql(statement_9, conn)\n",
    "    most_savvy_consignees = pd.read_sql(statement_10, conn)\n",
    "    \n",
    "except pyodbc.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('\\nConnection closed\\n')\n",
    "    else:\n",
    "        print('\\nNo connection was established\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consignee_name</th>\n",
       "      <th>avg_delta</th>\n",
       "      <th>total_shipments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JASON FURNITURE(HANGZHOU)CO.,LTD</td>\n",
       "      <td>-40.438389</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OCEANROADS GMBH</td>\n",
       "      <td>-29.454545</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VPA EXPORTERS</td>\n",
       "      <td>-28.936782</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMA S.P.A. VIA PUCCINI, 28</td>\n",
       "      <td>-27.313636</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUSYBEE KNITWEAR</td>\n",
       "      <td>-26.687688</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IGIFT COMPANY LIMITED</td>\n",
       "      <td>-26.650794</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TALLER NAGUISA S L</td>\n",
       "      <td>-25.901639</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SPENCER GOLDFARB</td>\n",
       "      <td>-25.839901</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YIWU WOQI PACKAGING CO LTD</td>\n",
       "      <td>-25.740113</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GIANT CASTLE CO., LTD.</td>\n",
       "      <td>-25.051075</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JIANGXI JINRUI HAIR PRODUCTS</td>\n",
       "      <td>-25.048780</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HAINING TEXTIMETEXTILE CO.,LTD.</td>\n",
       "      <td>-24.745223</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CH LIGHT TECHNOLOGY CO.,LTD.</td>\n",
       "      <td>-24.595556</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SHANGHAI JUFA AUTO PARTS CO., LTD</td>\n",
       "      <td>-23.963636</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MEDICHEM, S.A.</td>\n",
       "      <td>-23.962264</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       consignee_name  avg_delta  total_shipments\n",
       "0    JASON FURNITURE(HANGZHOU)CO.,LTD -40.438389              844\n",
       "1                     OCEANROADS GMBH -29.454545              110\n",
       "2                       VPA EXPORTERS -28.936782              174\n",
       "3          AMA S.P.A. VIA PUCCINI, 28 -27.313636              440\n",
       "4                    BUSYBEE KNITWEAR -26.687688              333\n",
       "5               IGIFT COMPANY LIMITED -26.650794              126\n",
       "6                  TALLER NAGUISA S L -25.901639              122\n",
       "7                    SPENCER GOLDFARB -25.839901              406\n",
       "8          YIWU WOQI PACKAGING CO LTD -25.740113              354\n",
       "9              GIANT CASTLE CO., LTD. -25.051075              372\n",
       "10       JIANGXI JINRUI HAIR PRODUCTS -25.048780              123\n",
       "11    HAINING TEXTIMETEXTILE CO.,LTD. -24.745223              157\n",
       "12       CH LIGHT TECHNOLOGY CO.,LTD. -24.595556              225\n",
       "13  SHANGHAI JUFA AUTO PARTS CO., LTD -23.963636              110\n",
       "14                     MEDICHEM, S.A. -23.962264              106"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_savvy_consignees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
